(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{150:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"rightToc",(function(){return c})),a.d(t,"default",(function(){return p}));a(58),a(31),a(22),a(23),a(59),a(0);var n=a(158),r=a(160);function i(){return(i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n])}return e}).apply(this,arguments)}var o={id:"overview",title:"Overview",sidebar_label:"Overview"},c=[],l={rightToc:c},b="wrapper";function p(e){var t=e.components,a=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,["components"]);return Object(n.b)(b,i({},l,a,{components:t,mdxType:"MDXLayout"}),Object(n.b)("p",null,"VizSeq is a visual analysis toolkit for text generation tasks like machine translation, summarization, image captioning,\nspeech translation and video description. "),Object(n.b)("p",null,"It takes multi-modal sources, text references as well as text model predictions as inputs, and analyzes them visually\nin ",Object(n.b)("a",i({parentName:"p"},{href:"ipynb"}),"Jupyter Notebook")," or in a built-in ",Object(n.b)("a",i({parentName:"p"},{href:"web_app"}),"Web App"),". It also provides a collection of multi-process scorers as a normal\nPython package."),Object(n.b)("p",{align:"center"},Object(n.b)("img",{src:Object(r.a)("img/overview.png"),alt:"VizSeq Overview",width:"480",class:"center"})),Object(n.b)("p",null,"Please also check out our ",Object(n.b)("a",i({parentName:"p"},{href:"https://arxiv.org/pdf/1909.05424.pdf"}),"EMNLP paper")," for more details."),Object(n.b)("h4",{id:"example-tasks"},"Example Tasks"),Object(n.b)("p",null,"The source modality can be text, image, audio, video or any combination of them. This covers a wide range of text\ngeneration tasks, examples of which are listed below:"),Object(n.b)("table",null,Object(n.b)("thead",{parentName:"table"},Object(n.b)("tr",{parentName:"thead"},Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Source"),Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Example Tasks"))),Object(n.b)("tbody",{parentName:"table"},Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Text"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Image"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Image captioning, image question answering, optical character recognition")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Audio"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Speech recognition, speech translation")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Video"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Video description")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal machine translation")))),Object(n.b)("h4",{id:"built-in-metrics"},"Built-in metrics"),Object(n.b)("table",null,Object(n.b)("thead",{parentName:"table"},Object(n.b)("tr",{parentName:"thead"},Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Type"),Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Metrics"))),Object(n.b)("tbody",{parentName:"table"},Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"N-gram-based *"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"BLEU (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/P02-1040"}),"Papineni et al., 2002"),"), NIST (",Object(n.b)("a",i({parentName:"td"},{href:"http://www.mt-archive.info/HLT-2002-Doddington.pdf"}),"Doddington, 2002"),"), METEOR (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W05-0909"}),"Banerjee et al., 2005"),"), TER (",Object(n.b)("a",i({parentName:"td"},{href:"http://mt-archive.info/AMTA-2006-Snover.pdf"}),"Snover et al., 2006"),"), RIBES (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/D10-1092"}),"Isozaki et al., 2010"),"), chrF (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W15-3049"}),"Popovi\u0107 et al., 2015"),"), GLEU (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1609.08144.pdf"}),"Wu et al., 2016"),"), ROUGE (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W04-1013"}),"Lin, 2004"),"), CIDEr (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf"}),"Vedantam et al., 2015"),"), WER")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Embedding-based"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"LASER (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1812.10464.pdf"}),"Artetxe and Schwenk, 2018"),"), BERTScore (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1904.09675.pdf"}),"Zhang et al., 2019"),")")))),Object(n.b)("p",null,"*"," ",Object(n.b)("strong",{parentName:"p"},"with multi-process acceleration")))}p.isMDXComponent=!0},158:function(e,t,a){"use strict";a.d(t,"a",(function(){return c})),a.d(t,"b",(function(){return s}));var n=a(0),r=a.n(n),i=r.a.createContext({}),o=function(e){var t=r.a.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):Object.assign({},t,e)),a},c=function(e){var t=o(e.components);return r.a.createElement(i.Provider,{value:t},e.children)};var l="mdxType",b={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},p=Object(n.forwardRef)((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,l=function(e,t){var a={};for(var n in e)Object.prototype.hasOwnProperty.call(e,n)&&-1===t.indexOf(n)&&(a[n]=e[n]);return a}(e,["components","mdxType","originalType","parentName"]),p=o(a),s=n,d=p[c+"."+s]||p[s]||b[s]||i;return a?r.a.createElement(d,Object.assign({},{ref:t},l,{components:a})):r.a.createElement(d,Object.assign({},{ref:t},l))}));function s(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=p;var c={};for(var b in t)hasOwnProperty.call(t,b)&&(c[b]=t[b]);c.originalType=e,c[l]="string"==typeof e?e:n,o[1]=c;for(var s=2;s<i;s++)o[s]=a[s];return r.a.createElement.apply(null,o)}return r.a.createElement.apply(null,a)}p.displayName="MDXCreateElement"},159:function(e,t,a){"use strict";var n=a(0),r=a(61);t.a=function(){return Object(n.useContext)(r.a)}},160:function(e,t,a){"use strict";a.d(t,"a",(function(){return r}));a(163);var n=a(159);function r(e){var t=(Object(n.a)().siteConfig||{}).baseUrl,a=void 0===t?"/":t;if(!e)return e;return/^(https?:|\/\/)/.test(e)?e:e.startsWith("/")?a+e.slice(1):a+e}},163:function(e,t,a){"use strict";var n=a(9),r=a(24),i=a(94),o="".startsWith;n(n.P+n.F*a(95)("startsWith"),"String",{startsWith:function(e){var t=i(this,e,"startsWith"),a=r(Math.min(arguments.length>1?arguments[1]:void 0,t.length)),n=String(e);return o?o.call(t,n,a):t.slice(a,a+n.length)===n}})}}]);