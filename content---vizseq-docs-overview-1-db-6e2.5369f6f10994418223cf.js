(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{148:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"rightToc",(function(){return c})),a.d(t,"default",(function(){return s}));a(59),a(32),a(23),a(24),a(60),a(0);var n=a(175),r=a(176);function i(){return(i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n])}return e}).apply(this,arguments)}var o={id:"overview",title:"Overview",sidebar_label:"Overview"},c=[{value:"Task Coverage",id:"task-coverage",children:[]},{value:"Metric Coverage",id:"metric-coverage",children:[]},{value:"License",id:"license",children:[]}],l={rightToc:c},b="wrapper";function s(e){var t=e.components,a=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,["components"]);return Object(n.b)(b,i({},l,a,{components:t,mdxType:"MDXLayout"}),Object(n.b)("p",null,"VizSeq is a visual analysis toolkit for text generation tasks like machine translation, summarization, image captioning,\nspeech translation and video description. "),Object(n.b)("p",null,"It takes multi-modal sources, text references as well as text model predictions as inputs, and analyzes them visually\nin ",Object(n.b)("a",{href:Object(r.a)("docs/getting_started/ipynb_example")},"Jupyter Notebook")," or in a built-in ",Object(n.b)("a",{href:Object(r.a)("docs/getting_started/web_app_example")},"Web App"),". It also provides a collection of\nmulti-process scorers as a normal Python package."),Object(n.b)("p",{align:"center"},Object(n.b)("img",{src:Object(r.a)("img/overview.png"),alt:"VizSeq Overview",width:"480",class:"center"})),Object(n.b)("p",null,"Please also see our ",Object(n.b)("a",{href:"https://arxiv.org/pdf/1909.05424.pdf",target:"_blank"},"paper")," for more details. To\ninstall VizSeq, check out the ",Object(n.b)("a",{href:Object(r.a)("docs/getting_started/installation")},"instructions")," here."),Object(n.b)("h2",{id:"task-coverage"},"Task Coverage"),Object(n.b)("p",null,"VizSeq accepts various source types, including text, image, audio, video or any combination of them. This covers a wide\nrange of text generation tasks, examples of which are listed below:"),Object(n.b)("table",null,Object(n.b)("thead",{parentName:"table"},Object(n.b)("tr",{parentName:"thead"},Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Source"),Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Example Tasks"))),Object(n.b)("tbody",{parentName:"table"},Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Text"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Image"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Image captioning, image question answering, optical character recognition")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Audio"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Speech recognition, speech translation")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Video"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Video description")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal machine translation")))),Object(n.b)("h2",{id:"metric-coverage"},"Metric Coverage"),Object(n.b)("p",null,Object(n.b)("strong",{parentName:"p"},"Accelerated with multi-processing/multi-threading.")),Object(n.b)("table",null,Object(n.b)("thead",{parentName:"table"},Object(n.b)("tr",{parentName:"thead"},Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Type"),Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"Metrics"))),Object(n.b)("tbody",{parentName:"table"},Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"N-gram-based"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"BLEU (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/P02-1040"}),"Papineni et al., 2002"),"), NIST (",Object(n.b)("a",i({parentName:"td"},{href:"http://www.mt-archive.info/HLT-2002-Doddington.pdf"}),"Doddington, 2002"),"), METEOR (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W05-0909"}),"Banerjee et al., 2005"),"), TER (",Object(n.b)("a",i({parentName:"td"},{href:"http://mt-archive.info/AMTA-2006-Snover.pdf"}),"Snover et al., 2006"),"), RIBES (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/D10-1092"}),"Isozaki et al., 2010"),"), chrF (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W15-3049"}),"Popovi\u0107 et al., 2015"),"), GLEU (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1609.08144.pdf"}),"Wu et al., 2016"),"), ROUGE (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W04-1013"}),"Lin, 2004"),"), CIDEr (",Object(n.b)("a",i({parentName:"td"},{href:"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf"}),"Vedantam et al., 2015"),"), WER")),Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"Embedding-based"),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"LASER (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1812.10464.pdf"}),"Artetxe and Schwenk, 2018"),"), BERTScore (",Object(n.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1904.09675.pdf"}),"Zhang et al., 2019"),")")))),Object(n.b)("h2",{id:"license"},"License"),Object(n.b)("p",null,"VizSeq is licensed under ",Object(n.b)("a",{href:"https://github.com/facebookresearch/vizseq/blob/master/LICENSE",target:"_blank"},"MIT"),"."))}s.isMDXComponent=!0},174:function(e,t,a){"use strict";var n=a(0),r=a(61);t.a=function(){return Object(n.useContext)(r.a)}},175:function(e,t,a){"use strict";a.d(t,"a",(function(){return c})),a.d(t,"b",(function(){return p}));var n=a(0),r=a.n(n),i=r.a.createContext({}),o=function(e){var t=r.a.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):Object.assign({},t,e)),a},c=function(e){var t=o(e.components);return r.a.createElement(i.Provider,{value:t},e.children)};var l="mdxType",b={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},s=Object(n.forwardRef)((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,l=function(e,t){var a={};for(var n in e)Object.prototype.hasOwnProperty.call(e,n)&&-1===t.indexOf(n)&&(a[n]=e[n]);return a}(e,["components","mdxType","originalType","parentName"]),s=o(a),p=n,d=s[c+"."+p]||s[p]||b[p]||i;return a?r.a.createElement(d,Object.assign({},{ref:t},l,{components:a})):r.a.createElement(d,Object.assign({},{ref:t},l))}));function p(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=s;var c={};for(var b in t)hasOwnProperty.call(t,b)&&(c[b]=t[b]);c.originalType=e,c[l]="string"==typeof e?e:n,o[1]=c;for(var p=2;p<i;p++)o[p]=a[p];return r.a.createElement.apply(null,o)}return r.a.createElement.apply(null,a)}s.displayName="MDXCreateElement"},176:function(e,t,a){"use strict";a.d(t,"a",(function(){return r}));a(177);var n=a(174);function r(e){var t=(Object(n.a)().siteConfig||{}).baseUrl,a=void 0===t?"/":t;if(!e)return e;return/^(https?:|\/\/)/.test(e)?e:e.startsWith("/")?a+e.slice(1):a+e}},177:function(e,t,a){"use strict";var n=a(9),r=a(25),i=a(94),o="".startsWith;n(n.P+n.F*a(95)("startsWith"),"String",{startsWith:function(e){var t=i(this,e,"startsWith"),a=r(Math.min(arguments.length>1?arguments[1]:void 0,t.length)),n=String(e);return o?o.call(t,n,a):t.slice(a,a+n.length)===n}})}}]);