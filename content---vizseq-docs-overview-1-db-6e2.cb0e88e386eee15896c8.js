(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{146:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"rightToc",(function(){return c})),a.d(t,"default",(function(){return p}));a(59),a(32),a(23),a(24),a(60),a(0);var r=a(166),n=a(168);function i(){return(i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var r in a)Object.prototype.hasOwnProperty.call(a,r)&&(e[r]=a[r])}return e}).apply(this,arguments)}var o={id:"overview",title:"Overview",sidebar_label:"Overview"},c=[{value:"Task Coverage",id:"task-coverage",children:[]},{value:"Metric Coverage",id:"metric-coverage",children:[]},{value:"License",id:"license",children:[]}],l={rightToc:c},b="wrapper";function p(e){var t=e.components,a=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,["components"]);return Object(r.b)(b,i({},l,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"VizSeq is a visual analysis toolkit for text generation tasks like machine translation, summarization, image captioning,\nspeech translation and video description. "),Object(r.b)("p",null,"It takes multi-modal sources, text references as well as text model predictions as inputs, and analyzes them visually\nin ",Object(r.b)("a",i({parentName:"p"},{href:"getting_started/ipynb_example"}),"Jupyter Notebook")," or in a built-in ",Object(r.b)("a",i({parentName:"p"},{href:"getting_started/web_app_example"}),"Web App"),". It also provides a collection of multi-process scorers as a normal\nPython package."),Object(r.b)("p",{align:"center"},Object(r.b)("img",{src:Object(n.a)("img/overview.png"),alt:"VizSeq Overview",width:"480",class:"center"})),Object(r.b)("p",null,"Please also see our ",Object(r.b)("a",i({parentName:"p"},{href:"https://arxiv.org/pdf/1909.05424.pdf"}),"paper")," for more details. To install VizSeq, check out the ",Object(r.b)("a",i({parentName:"p"},{href:"getting_started/installation"}),"instructions")," here."),Object(r.b)("h2",{id:"task-coverage"},"Task Coverage"),Object(r.b)("p",null,"VizSeq accepts various source types, including text, image, audio, video or any combination of them. This covers a wide range of text\ngeneration tasks, examples of which are listed below:"),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",i({parentName:"tr"},{align:"left"}),"Source"),Object(r.b)("th",i({parentName:"tr"},{align:"left"}),"Example Tasks"))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Text"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Image"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Image captioning, image question answering, optical character recognition")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Audio"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Speech recognition, speech translation")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Video"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Video description")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Multimodal machine translation")))),Object(r.b)("h2",{id:"metric-coverage"},"Metric Coverage"),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Accelerated with multi-processing/multi-threading.")),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",i({parentName:"tr"},{align:"left"}),"Type"),Object(r.b)("th",i({parentName:"tr"},{align:"left"}),"Metrics"))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"N-gram-based"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"BLEU (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/P02-1040"}),"Papineni et al., 2002"),"), NIST (",Object(r.b)("a",i({parentName:"td"},{href:"http://www.mt-archive.info/HLT-2002-Doddington.pdf"}),"Doddington, 2002"),"), METEOR (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W05-0909"}),"Banerjee et al., 2005"),"), TER (",Object(r.b)("a",i({parentName:"td"},{href:"http://mt-archive.info/AMTA-2006-Snover.pdf"}),"Snover et al., 2006"),"), RIBES (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/D10-1092"}),"Isozaki et al., 2010"),"), chrF (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W15-3049"}),"Popovi\u0107 et al., 2015"),"), GLEU (",Object(r.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1609.08144.pdf"}),"Wu et al., 2016"),"), ROUGE (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.aclweb.org/anthology/W04-1013"}),"Lin, 2004"),"), CIDEr (",Object(r.b)("a",i({parentName:"td"},{href:"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf"}),"Vedantam et al., 2015"),"), WER")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"Embedding-based"),Object(r.b)("td",i({parentName:"tr"},{align:"left"}),"LASER (",Object(r.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1812.10464.pdf"}),"Artetxe and Schwenk, 2018"),"), BERTScore (",Object(r.b)("a",i({parentName:"td"},{href:"https://arxiv.org/pdf/1904.09675.pdf"}),"Zhang et al., 2019"),")")))),Object(r.b)("h2",{id:"license"},"License"),Object(r.b)("p",null,"VizSeq is licensed under ",Object(r.b)("a",i({parentName:"p"},{href:"https://github.com/facebookresearch/vizseq/blob/master/LICENSE"}),"MIT"),"."))}p.isMDXComponent=!0},166:function(e,t,a){"use strict";a.d(t,"a",(function(){return c})),a.d(t,"b",(function(){return s}));var r=a(0),n=a.n(r),i=n.a.createContext({}),o=function(e){var t=n.a.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):Object.assign({},t,e)),a},c=function(e){var t=o(e.components);return n.a.createElement(i.Provider,{value:t},e.children)};var l="mdxType",b={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},p=Object(r.forwardRef)((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,l=function(e,t){var a={};for(var r in e)Object.prototype.hasOwnProperty.call(e,r)&&-1===t.indexOf(r)&&(a[r]=e[r]);return a}(e,["components","mdxType","originalType","parentName"]),p=o(a),s=r,d=p[c+"."+s]||p[s]||b[s]||i;return a?n.a.createElement(d,Object.assign({},{ref:t},l,{components:a})):n.a.createElement(d,Object.assign({},{ref:t},l))}));function s(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=p;var c={};for(var b in t)hasOwnProperty.call(t,b)&&(c[b]=t[b]);c.originalType=e,c[l]="string"==typeof e?e:r,o[1]=c;for(var s=2;s<i;s++)o[s]=a[s];return n.a.createElement.apply(null,o)}return n.a.createElement.apply(null,a)}p.displayName="MDXCreateElement"},167:function(e,t,a){"use strict";var r=a(0),n=a(61);t.a=function(){return Object(r.useContext)(n.a)}},168:function(e,t,a){"use strict";a.d(t,"a",(function(){return n}));a(169);var r=a(167);function n(e){var t=(Object(r.a)().siteConfig||{}).baseUrl,a=void 0===t?"/":t;if(!e)return e;return/^(https?:|\/\/)/.test(e)?e:e.startsWith("/")?a+e.slice(1):a+e}},169:function(e,t,a){"use strict";var r=a(9),n=a(25),i=a(94),o="".startsWith;r(r.P+r.F*a(95)("startsWith"),"String",{startsWith:function(e){var t=i(this,e,"startsWith"),a=n(Math.min(arguments.length>1?arguments[1]:void 0,t.length)),r=String(e);return o?o.call(t,r,a):t.slice(a,a+r.length)===r}})}}]);